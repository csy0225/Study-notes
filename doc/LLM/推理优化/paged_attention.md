# PagedAttention
是一种处理 kv cache 的内存管理算法，减少显存碎片的出现，提高内存利用率。
+ PagedAttention(vLLM):更快地推理你的GPT https://juejin.cn/post/7259249904778018853